{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework assignment 4 - Classification"
      ],
      "metadata": {
        "id": "KUviJ0EjYJlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you begin\n",
        "\n",
        "Remember to:\n",
        "\n",
        "1. Make your own copy of the notebook by pressing the \"Copy to drive\" button.\n",
        "2. Expend all cells by pressing **Ctrl+[**\n",
        "\n",
        "## Tip of the day - Displaying functions documentation\n",
        "\n",
        "You can quickly display a function's documentation by pressing **Alt+/** when standing on it with the cursor.\n",
        "\n",
        "You can also open a small documentation window at the bottom of the screen by running a command for the format of **?{function}** in a new cell (and replacing **{function}** with your function's name.\n",
        "\n",
        "Try opening a new cell, bellow this one by clicking on the **+code** button below the menu bar. Then type:\n",
        "```python\n",
        "?print\n",
        "```\n",
        "into it and run it.\n",
        "\n",
        "You would need to use the functions' full call string. For example, to view the documentation of the **randint** function in the numpy package, you will have to run *?np.random.randint*. You can, of course, only view the documentation for this function after importing the numpy library (i.e., after running *import numpy as np*)\n",
        "\n",
        "### Your IDs\n",
        "\n",
        "✍️ Fill in your IDs in the cell below:"
      ],
      "metadata": {
        "id": "zrfs83BjYJlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Fill in your IDs (as a string)\n",
        "student1_id = '...'\n",
        "student2_id = '...'\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print('Hello ' + student1_id + ' & ' + student2_id)"
      ],
      "metadata": {
        "id": "I1HpBWnXYJlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Packages\n",
        "\n",
        "Importing the NumPy, Pandas and Matplotlib packages."
      ],
      "metadata": {
        "id": "KP3oitGuYJlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## This line makes matplotlib plot the figures inside the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "## Set some default values of the the matplotlib plots\n",
        "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
        "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
      ],
      "metadata": {
        "id": "Kocak-pxYJlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Digits Dataset\n",
        "\n",
        "For this task, we will use a dataset of 1797 digits collected from 43 people. Each digit is an image of 8x8 pixels taking integer values between 0 and 16. More details about the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits).\n",
        "\n",
        "To load the data, we will use the scikit-learn's function [sklearn.datasets.load_digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)."
      ],
      "metadata": {
        "id": "y2KgZ5tiYJlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "dataset = load_digits()\n",
        "\n",
        "x = dataset.images\n",
        "y = dataset.target\n",
        "\n",
        "print('Number of images in the dataset: {}'.format(len(x)))\n",
        "print('Each images size is: {}'.format(x.shape[1:]))\n",
        "print('These are the first 80 images:')\n",
        "\n",
        "fig, ax_array = plt.subplots(10, 8)\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    ax.imshow(x[i], cmap='gray')\n",
        "    ax.set_ylabel(y[i])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])"
      ],
      "metadata": {
        "id": "unpDsxeJYJlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Validation-Test split\n",
        "\n",
        "✍️ Complete the code below to split the data into 60% train 20% validation set set and 20% test set"
      ],
      "metadata": {
        "id": "cwUeeSp8YJlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = x.shape[0]  # The total number of samples in the dataset\n",
        "\n",
        "## Generate a random generator with a fixed seed\n",
        "rand_gen = np.random.RandomState(0)\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Generating a shuffled vector of indices\n",
        "...\n",
        "\n",
        "## Split the indices into 60% train / 20% validation / 20% test\n",
        "...\n",
        "train_indices = ...\n",
        "val_indices = ...\n",
        "test_indices = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "## Extract the sub datasets from the full dataset using the calculated indices\n",
        "x_train = x[train_indices]\n",
        "y_train = y[train_indices]\n",
        "x_val = x[val_indices]\n",
        "y_val = y[val_indices]\n",
        "x_test = x[test_indices]\n",
        "y_test = y[test_indices]"
      ],
      "metadata": {
        "id": "XXp2YC1nYJlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The task"
      ],
      "metadata": {
        "id": "C3bKwZTNYJlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our task here is to be able to predict the correct label (name of the person) given an image of his face. Formally, we would like to find a classifier $h(\\boldsymbol{x})$, which would minimize the misclassification rate:\n",
        "$$\n",
        "R\\{h\\}=E\\left[I\\{h(\\boldsymbol{x})\\neq y\\}\\right]\n",
        "$$\n",
        "\n",
        "Here $\\boldsymbol{x}$ is the measured data, which is in our case the images, and $y$ is the label of the image.\n",
        "\n",
        "✍️ Complete the code below so that it will compute the misclassification rate of a given predictor on a given dataset."
      ],
      "metadata": {
        "id": "zQGKDW7kYJlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_score(h_func, x, y):\n",
        "    \"\"\"\n",
        "    Calculates the misclassification rate of a predictor on a given dataset.\n",
        "\n",
        "    Using:\n",
        "    - N: the number of samples in the dataset.\n",
        "    - ImageSize: the size of the image (in our case 8x8).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    h_func: a function\n",
        "        The prediction function which recived an input x and produces a prediction y_hat\n",
        "    x: ndarray\n",
        "        The NxImageSize array of the features of the dataset.\n",
        "    y: ndarray\n",
        "        The 1D array of length N of the labels of the dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    score: scalar\n",
        "        The evaluated score on the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    y_hat = ...\n",
        "    score = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return score\n",
        "\n",
        "## Test the function on a prediction function which always output 1\n",
        "h_func = lambda x: np.ones(x.shape[0], dtype=int)\n",
        "\n",
        "print(f'The score on the test set is: {calc_score(h_func, x_test, y_test):.4f}')"
      ],
      "metadata": {
        "id": "-Cd_c21yYJlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-NN Classification\n",
        "\n",
        "Lets us now try to classify the images using 1-nearest neighbor (1-NN).\n",
        "\n",
        "✍️ Complete the following code to implement the 1-NN classification.\n",
        "\n",
        "- Use the [cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) function, which we encountered in the last assignment, to calculate the matrix of all distances between two sets of vectors."
      ],
      "metadata": {
        "id": "N6OOgiS4YJlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def one_nn(x, x_train, y_train):\n",
        "    \"\"\"\n",
        "    Calculates the estimated labels for a given set of features using the 1-NN method.\n",
        "\n",
        "    Using:\n",
        "    - N: the number of samples in the train set.\n",
        "    - M: the number of samples for which the labels are to be estimated.\n",
        "    - ImageSize: the size of the image (in our case 8x8).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "        The MxImageSize 2D array of features for which the labels are to be estimated.\n",
        "    x_train: ndarray\n",
        "        The NxImageSize 2D array of the features of the train set.\n",
        "    y_train: ndarray\n",
        "        The 1D array of length N of the labels of the train set.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_hat: ndarray\n",
        "        The 1D array of length M of the estimated labels.\n",
        "    \"\"\"\n",
        "\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    dists = ...\n",
        "    y_hat = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "## Define a prediction function\n",
        "h_func = lambda x: one_nn(x, x_train, y_train)\n",
        "\n",
        "## Calcualte the score on the validation set\n",
        "print(f'The score on the validation set is: {calc_score(h_func, x_val, y_val):.4f}')\n",
        "\n",
        "## Plot the prediction\n",
        "fig, ax_array = plt.subplots(10, 8)\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    ax.imshow(x_val[i], cmap='gray')\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "    y_hat = h_func(x_val[i:(i+1)])[0]\n",
        "    ax.set_ylabel(f'{y_hat} ({y_val[i]})',\n",
        "                     color='black' if y_hat == y_val[i] else 'red')\n",
        "fig.suptitle('Predicted (Correct) Digits; Incorrect in Red', size=14);"
      ],
      "metadata": {
        "id": "0rqC9BIIYJlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get a score of below 5% on the validation set."
      ],
      "metadata": {
        "id": "lJ9m1UbKYJla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearn.neighbors.KNeighborsClassifier\n",
        "\n",
        "The class [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) implements the K-NN algorithm.\n",
        "\n",
        "✍️ Read the function's documentation and fill in the following code run 1-NN using scikit-learn's class.\n",
        "\n",
        "- You will need to reshape the data from Nx8x8 to Nx64."
      ],
      "metadata": {
        "id": "6-XeK-szYJld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "classifier = KNeighborsClassifier(...\n",
        "classifier.fit(...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "h_func = lambda x: classifier.predict(x.reshape(x.shape[0], -1))\n",
        "\n",
        "print(f'The score on the validation set is: {calc_score(h_func, x_val, y_val):.4f}')"
      ],
      "metadata": {
        "id": "TmAzndXIYJle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get more or less the same score as you got before."
      ],
      "metadata": {
        "id": "xmBIouT5YJlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting K\n",
        "\n",
        "We will use the validation set to select K.\n",
        "\n",
        "✍️ Complete the following code to calculate the model's score on the train set and the validation set for each value of K in the given list of Ks:"
      ],
      "metadata": {
        "id": "Hn3z1G9SYJlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = np.arange(1, 30)\n",
        "\n",
        "train_scores = {}\n",
        "val_scores = {}\n",
        "\n",
        "for k in k_values:\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    classifier = ...\n",
        "    classifier.fit(...\n",
        "    h_func = lambda x: ...\n",
        "\n",
        "    train_scores[k] = calc_score(...\n",
        "    val_scores[k] = calc_score(...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(list(train_scores.keys()), list(train_scores.values()), label='Train')\n",
        "ax.plot(list(val_scores.keys()), list(val_scores.values()), label='Validation')\n",
        "ax.set_ylabel('Misclass. rate')\n",
        "ax.set_xlabel('$K$')\n",
        "ax.set_title('Score vs. K')\n",
        "ax.legend();"
      ],
      "metadata": {
        "id": "a3xgyVICYJlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code selects the K which produces the minimal score in the validation and calculates the score for K-NN with the selected K."
      ],
      "metadata": {
        "id": "k4f_sl6ZYJlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_k = min(val_scores.keys(), key=val_scores.get)\n",
        "print(f'The best K is: {best_k}, with a validation score of {val_scores[best_k]:.4f}')"
      ],
      "metadata": {
        "id": "HETTA3lQYJlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the way you have splitted the data you can get different Ks but it should be relatively small."
      ],
      "metadata": {
        "id": "Ah52LF59YJlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA\n",
        "\n",
        "We will now try to use linear discrimination analysis (LDA) method.\n",
        "\n",
        "✍️ Fill in the code below to create an LDA class which can train a model and use it to make predictions.\n",
        "\n",
        "- Reminder, the model's parameters are:\n",
        "  - The mean values for each class\n",
        "  - The covariance matrix for all classes.\n",
        "  - The prior distribution of each class.\n",
        "- The optimal prediction for the misclassification rate is:\n",
        "  $$\n",
        "  h(\\boldsymbol{x})=\\underset{y}{\\arg\\max}\\ p_{\\mathbf{x}|\\text{y}}(\\boldsymbol{x}|y)p_{\\text{y}}(y)\n",
        "  $$\n",
        "  - *Optional*: The term $\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}\\right)^T\\Sigma^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}\\right)$, which appears in $p\\left(\\boldsymbol{x}|y\\right)$, is called the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) between $\\boldsymbol{x}$ and $\\boldsymbol{\\mu}$ based on the covariance matrix $\\Sigma$. You can use the **cdist** function to calculate all the Mahalanobis distances between a set of $\\boldsymbol{x}$'s and a set of $\\boldsymbol{\\mu}$'s. This can be done using the **'mahalanobis'** metric and adding a **VI = $\\Sigma^{-1}$** arguments to the **cdist** function."
      ],
      "metadata": {
        "id": "gBBt9qoJYJli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LDA:\n",
        "    def __init__(self, n_classes=10):\n",
        "        \"\"\"\n",
        "        Initizaling the class fields.\n",
        "\n",
        "        Using:\n",
        "        - C: the number or classes.\n",
        "        - ImageSize: the size of the image (in our case 8x8).\n",
        "        - D: the dimension of the input (64).\n",
        "        \"\"\"\n",
        "        self.n_classes = n_classes  # C: The number of classes (digits).\n",
        "\n",
        "        ## These fields are initizalized with None and then learned in the fit method.\n",
        "        self.py = None  # The prior distribution on y.\n",
        "        self.mu = None  # An CxD matirx contating the mean value vecor for each class.\n",
        "        self.cov_mat = None  # The covaraince matrix of the Gaussian for all the classes\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"\n",
        "        Calculates the parameters of the LDA model.\n",
        "\n",
        "        Using:\n",
        "        - C: the number of classes.\n",
        "        - N: the number of samples in x.\n",
        "        - ImageSize: the size of the image (in our case 8x8).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: ndarray\n",
        "            The NxImageSize 2D array of features the train set.\n",
        "        y: ndarray\n",
        "            The 1D array of length N of the labels of the train set.\n",
        "        \"\"\"\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        x = x.reshape(...\n",
        "\n",
        "        self.py = ...\n",
        "        self.mu = ...\n",
        "        self.cov_mat = ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Calculates the parameters of the LDA model.\n",
        "\n",
        "        Using:\n",
        "        - C: the number of classes.\n",
        "        - N: the number of samples in x.\n",
        "        - ImageSize: the size of the image (in our case 8x8).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: ndarray\n",
        "            The NxImageSize 2D array of features for which the labels are to be estimated.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: ndarray\n",
        "            The 1D array of length N of the estimated labels.\n",
        "        \"\"\"\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        x = x.reshape(...\n",
        "        ...\n",
        "        y_hat = ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "## Train the model on the train set\n",
        "classifier = LDA()\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "## Define the prediction function\n",
        "h_func = classifier.predict\n",
        "\n",
        "## Plot P(y)\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(range(classifier.n_classes), classifier.py)\n",
        "ax.set_title(r'$p_{\\text{y}}(y)$')\n",
        "ax.set_xlabel('y')\n",
        "ax.set_ylabel('Probability')\n",
        "\n",
        "## Display the means\n",
        "fig, ax_array = plt.subplots(2, 5, figsize=(8.0, 4.0))\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    img = classifier.mu[i].reshape(x_train.shape[1:])\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_ylabel(i)\n",
        "fig.suptitle('Mean image', size=14);\n",
        "fig.tight_layout()\n",
        "\n",
        "## Calculate the model's score on the the validation set\n",
        "print(f'The score on the validation set is: {calc_score(h_func, x_val, y_val):.4f}')\n",
        "\n",
        "## Plot estimation\n",
        "fig, ax_array = plt.subplots(10, 8)\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    ax.imshow(x_val[i], cmap='gray')\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "    y_hat = h_func(x_val[i:(i+1)])[0]\n",
        "    ax.set_ylabel(f'{y_hat} ({y_val[i]})',\n",
        "                     color='black' if y_hat == y_val[i] else 'red')\n",
        "fig.suptitle('Predicted (Correct) Digits; Incorrect in Red', size=14)\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "kDe02JwzYJlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get a score of below 5% on the validation set."
      ],
      "metadata": {
        "id": "cfDKIwzLYJll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
        "\n",
        "The class [sklearn.discriminant_analysis.LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) implements the LDA algorithm.\n",
        "\n",
        "✍️ Read the function's documentation and fill in the following code run LDA using scikit-learn's class."
      ],
      "metadata": {
        "id": "7umBphBeYJll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "classifier = LinearDiscriminantAnalysis(...\n",
        "classifier.fit(...\n",
        "h_func = lambda x: ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print(f'The score on the validation set is: {calc_score(h_func, x_val, y_val):.4f}')"
      ],
      "metadata": {
        "id": "ye0PKJILYJlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Logistic Regression\n",
        "\n",
        "We will now try to use linear logistic regression:\n",
        "\n",
        "$$\n",
        "p_{\\text{y}|\\mathbf{x}}(y|\\boldsymbol{x})=\\frac{e^{\\boldsymbol{x}^{\\top}\\boldsymbol{\\theta}_{y}}}{\\sum_c e^{\\boldsymbol{x}^{\\top}\\boldsymbol{\\theta}_{c}}}\n",
        "$$\n",
        "\n",
        "✍️ Fill in the code below to create an linear logistic regression class which can train a model and use it to make predictions.\n",
        "\n",
        "- Reminder, the model's parameters are $C$ vectors $\\boldsymbol{\\theta}_c$ for each class of the.\n",
        "- In practice the class stores the parameters in a single matrix $\\Theta$ which contains all the $\\boldsymbol{\\theta}_c$ vectors:\n",
        "  $$\n",
        "  \\Theta=\n",
        "  \\begin{bmatrix}\n",
        "    |&|&&|\\\\\n",
        "    \\boldsymbol{\\theta}_1&\\boldsymbol{\\theta}_2&\\dots&\\boldsymbol{\\theta}_C\\\\\n",
        "    |&|&&|\\\\\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "- The parameters are learned by MLE (maximizing the log likelihood) using gradient descent.\n",
        "- The optimal prediction for the misclassification rate is:\n",
        "  $$\n",
        "  h(\\boldsymbol{x})=\\underset{y}{\\arg\\max}\\ p_{\\text{y}|\\mathbf{x}}(y|\\boldsymbol{x})=\\underset{y}{\\arg\\max}\\ \\boldsymbol{x}^{\\top}\\boldsymbol{\\theta}_{y}\n",
        "  $$\n",
        "- You do not need to implement the method calculating the gradient of the log likelihood. The given calc_grad_log_likelihood implements following formula:\n",
        "  $$\n",
        "  \\frac{1}{N}\\nabla_{\\Theta}\\log(\\mathcal{L})=\n",
        "  \\frac{1}{N}\\sum_i \\boldsymbol{x}^{(i)} \\left(\\boldsymbol{n}(y^{(i)})-\\text{softmax}(\\boldsymbol{x}^{(i)})^{\\top}\\right)\n",
        "  $$\n",
        "  where $\\boldsymbol{n}(c)$ is a vector of zeros except of a 1 at the $c$ position (for example $\\boldsymbol{n}(2)=[0,1,0,\\dots,0]^{\\top}$). (This is known as a one-hot representation).\n",
        "- You also do not need to implement it but the methods add a columns of ones to $X$ in order to include a bias term in the model."
      ],
      "metadata": {
        "id": "oPSkFzVLYJln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLogisticRegression:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initizaling the class fields.\n",
        "\n",
        "        Using:\n",
        "        - C: the number or classes.\n",
        "        - ImageSize: the size of the image (in our case 8x8).\n",
        "        - D: the dimension of the input (64).\n",
        "        \"\"\"\n",
        "\n",
        "        ## These fields is initizalized with None and then learned in the fit method.\n",
        "        self.theta = None  # A DxC matrix containing the C theta vectors.\n",
        "\n",
        "    def calc_prob(self, x):\n",
        "        \"\"\"\n",
        "        An auxiliary function for calcualting the probabilities of all classes for a given x.\n",
        "\n",
        "        Using:\n",
        "        - N: the number of samples in x.\n",
        "        - D: the dimention of x (64)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: ndarray\n",
        "            The NxD 2D array of N vectors of x.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        prob: ndarray\n",
        "            A NxC 2D array of N probability vectors for each of the N vectors of x.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        This function should in fact calculate the softmax function for each row in x.\n",
        "        \"\"\"\n",
        "\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        z = x @ self.theta\n",
        "        ...\n",
        "        prob = ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "        return prob\n",
        "\n",
        "    def log_likelihood(self, x, y):\n",
        "        \"\"\"\n",
        "        An auxiliary function for calcualting the log likelihood !!! divided by the number of samples !!!.\n",
        "\n",
        "        Using:\n",
        "        - N: the number of samples in x.\n",
        "        - D: the dimention of x (64)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: ndarray\n",
        "            The NxD 2D array of N vectors of x.\n",
        "        y: ndarray\n",
        "            The 1D array of length N of the labels of the train set.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        log_likelihood: float\n",
        "            The log likelihood of the dataset divided by the number of samples.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Use the calc_prob method.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        ...\n",
        "        log_likelihood = ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "        return log_likelihood\n",
        "\n",
        "    def calc_grad_log_likelihood(self, x, y):\n",
        "        temp_mat = -self.calc_prob(x)\n",
        "        temp_mat[np.arange(len(y)), y] += 1\n",
        "        grad = x.T @ temp_mat / x.shape[0]\n",
        "        return grad\n",
        "\n",
        "    def fit(self, x, y, eta, n_iters):\n",
        "        \"\"\"\n",
        "        Fitting the models parameters using gradient descent.\n",
        "\n",
        "        Using:\n",
        "        - N: the number of samples in x.\n",
        "        - ImageSize: the size of the image (in our case 8x8).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: ndarray\n",
        "            The NxImageSize 2D array of features the train set.\n",
        "        y: ndarray\n",
        "            The 1D array of length N of the labels of the train set.\n",
        "        eta: float\n",
        "            The learning rate of the gradient descent algorithm.\n",
        "        n_iters:\n",
        "            The number of iterations in the gradent descent.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        objective_list: ndarray\n",
        "            A n_iters 1D array of the intermidiate values of the objective (the minus log-likelihood).\n",
        "        \"\"\"\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        ## Add a column of ones to add a bias term\n",
        "        x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
        "\n",
        "        ## Initialize theta\n",
        "        self.theta = np.zeros((x.shape[1], y.max() + 1))\n",
        "\n",
        "        ## Initialize lists to store intermidiate results for plotting\n",
        "        objective_list = [-self.log_likelihood(x, y)]\n",
        "\n",
        "        ## Perforing the update steps\n",
        "        for i_iter in range(1, n_iters + 1):\n",
        "            ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "            ## Update theta\n",
        "            self.theta += eta * ...\n",
        "            ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "            ## Store intermidiate results\n",
        "            objective_list.append(-self.log_likelihood(x, y))\n",
        "\n",
        "        objective_list = np.array(objective_list)\n",
        "\n",
        "        return objective_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the prediction for a given set of x's\n",
        "\n",
        "        Using:\n",
        "        - N: the number of samples in x.\n",
        "        - ImageSize: the size of the image (in our case 8x8).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: ndarray\n",
        "            The NxImageSize 2D array of N vectors of x's.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: ndarray\n",
        "            A 1D array of N predictions for each of the N vectors of x.\n",
        "        \"\"\"\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
        "\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        y_hat = ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "## Test the class\n",
        "eta = 1e-4\n",
        "n_iters = 100\n",
        "classifier = LinearLogisticRegression()\n",
        "objective_list = classifier.fit(x_train, y_train, eta, n_iters)\n",
        "\n",
        "## Plot\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.plot(np.arange(len(objective_list)), objective_list)\n",
        "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Objective')"
      ],
      "metadata": {
        "id": "VNunwhiCYJln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see a graph of the objective starting at around 2.3 going down to about 1.9."
      ],
      "metadata": {
        "id": "48Fl18s-YJlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting the learning rate\n",
        "\n",
        "In order to select the learning rate we will test 4 values of $\\eta$ and plot the objective in the first 100 iterations. The following code does that."
      ],
      "metadata": {
        "id": "R_mrlVjEYJlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 100\n",
        "etas_list = (1e0, 1e-1, 1e-2, 1e-3)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
        "classifier = LinearLogisticRegression()\n",
        "for i_eta, eta in enumerate(etas_list):\n",
        "    objective_list = classifier.fit(x_train, y_train, eta, n_iters)\n",
        "\n",
        "    ## Plot\n",
        "    ax = axes.flat[i_eta]\n",
        "    ax.plot(np.arange(len(objective_list)), objective_list)\n",
        "    ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
        "    ax.set_xlabel('Step')\n",
        "    ax.set_ylabel('Objective')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "gF6lngQRYJlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the learning rate which monotonically decays (this criteria is relevant for the simple version of gradient\n",
        "of gradient descent implemented here, it is more complicated of more sophisticated versions of gradient descent).\n",
        "\n",
        "✍️ Fill in the code below to select the learning rate and retrain the model for 50000 iterations.\n",
        "\n",
        "- Running the code should take about 2 minutes"
      ],
      "metadata": {
        "id": "d1OsKcDiYJlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "eta = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "n_iters = 50000\n",
        "classifier = LinearLogisticRegression()\n",
        "objective_list = classifier.fit(x_train, y_train, eta, n_iters)\n",
        "\n",
        "## Plot\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.plot(np.arange(len(objective_list)), objective_list)\n",
        "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Objective')"
      ],
      "metadata": {
        "id": "-o6MZ81aYJlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model"
      ],
      "metadata": {
        "id": "bq8TZO-rYJlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_func = lambda x: classifier.predict(x)\n",
        "\n",
        "print(f'The score on the validation set is: {calc_score(h_func, x_val, y_val):.4f}')\n",
        "\n",
        "## Plot estimation\n",
        "fig, ax_array = plt.subplots(10, 8)\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    ax.imshow(x_val[i], cmap='gray')\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "    y_hat = h_func(x_val[i:(i+1)])[0]\n",
        "    ax.set_ylabel(f'{y_hat} ({y_val[i]})',\n",
        "                     color='black' if y_hat == y_val[i] else 'red')\n",
        "fig.suptitle('Predicted (Correct) Digits; Incorrect in Red', size=14)\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "SqXAq9qJYJlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get a score of below 5% on the validation set."
      ],
      "metadata": {
        "id": "M6ZJGz8FYJlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting the best model\n",
        "\n",
        "✍️ Copy the code which generates the prediction function with the best results in the validation set."
      ],
      "metadata": {
        "id": "5bTHltz0YJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "...\n",
        "h_func = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print(f'The score on the validation set is: {calc_score(h_func, x_val, y_val):.4f}')\n",
        "print(f'The score on the test set is: {calc_score(h_func, x_test, y_test):.4f}')"
      ],
      "metadata": {
        "id": "EvMU42hTYJlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminder: Submission\n",
        "\n",
        "To submit your code, download it as a **ipynb** file from Colab, and upload it to the course's website. You can download this code by selecting **Download .ipynb** from the **file** menu."
      ],
      "metadata": {
        "id": "YKfAI5R4YJlx"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}