---
template: page
---

<div dir="rtl" class="site-style">

# Gradient descent (אלגוריתם הגרדיאנט)

המקרה של LLS, שבו יש לבעיית האופטימיזציה פתרון סגור, הוא למעשה לא מאד מייצג ובמרבית המקרים נצטרך למצוא דרך אחרת לפתור את בעיית האופטימיזציה.

לדוגמא, נחזור שוב לבעיית חיזוי זמן הנסיעה. נניח ואנו רוצים לנסות להשתמש במודל מהצורה:

$$
h(x;\boldsymbol{\theta})=\theta_1-\theta_2 \exp(-x/\theta_3)
$$

(מודל אשר מתחיל ב $(\theta_1-\theta_2)$ ומתכנס ל $\theta_1$ בקצב אקספוננציאלי עם פרמטר $\theta_3$)

נרצה למצוא את הפרמטרים האופטימאליים של מודל זה בשיטת ERM תחת פונקציית המחיר של RMSE. בעיית האופטימיזציה במקרה זה תהיה:

$$
\boldsymbol{\theta}^*_{\mathcal{D}}
=\underset{\boldsymbol{\theta}}{\arg\min} \sqrt{\frac{1}{N}\sum_{i=0}^N(\theta_1-\theta_2 \exp(-x/\theta_3)-y_i)^2}
$$

ניתן כמובן להיפתר מהשורש ומה $\frac{1}{N}$ מבלי לשנות את בעיית האופטימיזציה:

$$
\boldsymbol{\theta}^*_{\mathcal{D}}
=\underset{\boldsymbol{\theta}}{\arg\min} \underbrace{\sum_{i=0}^N(\theta_1-\theta_2 \exp(-x/\theta_3)-y_i)^2}_{f(\boldsymbol{\theta};\mathcal{D})}
$$

לשם הנוחות סימנו את פונקציית המטרה (שאותה אנו רוצים למזער) ב $f(\boldsymbol{\theta};\mathcal{D})$

מכיוון שבבעיה זו לא ניתן להגיע לפתרון סגור על ידי גזירה של פונקציית המטרה והשוואה ל-0 נאלץ להשתמש בשיטות אחרות. אחת השיטות הנפוצות במערכות לומדות (בעיקר בעבודה עם רשתות נוירונים, אך לא רק) לפתרון בעיות אופטימיזציה מסוג זה הינה להשתמש ב gradient descent אותו הצגנו בקצרה בתרגול 1. בהרצאה זו רק נחזור על אופן פעולת האלגוריתם. בהמשך הקורס אנו נדון בבעיות של אלגוריתם זה וכיצד ניתן להתמודד איתן.

הרעיון מאחרי Gradient descent הוא פשוט. האלגוריתם מנסה למצוא מינימום לוקאלי של פונקציית המטרה על ידי התקדמות בצעדים קטנים בכיוון שבו הפונקציה יורדת הכי מהר. אילוסטרציה:

<div class="imgbox">

![](./assets/sled.jpg)

</div>

לאלגוריתמים אטרטיביים מסוג זה, אשר מנסים בכל איטרציה לשפר את מצבם לעומת המצב הנוכחי (מבלי התייחס צורה הגלובאלית של הפונקציה) אנו קוראים אלגוריתמים חמדנים (greedy). כפי שציינו קודם אלגוריתמים כאלה לא מתיימרים להתכנס לאפטימום הגלובאלי, אלא רק ינסו להשתפר כל הזמן עד אשר יגיעו לאופטימום לוקאלי.

הדרישה היחידה על הבעיה לשםהשימוש באלגוריתם הינה היכולת לחשב את הנגזרת של פונקציית המטרה. בהינתן הנגזרת שלבי האלגוריתם הם כדלקמן:

- מאתחלים את הפרמטרים שלפיהם רוצים למצוא את המינימום, במקרה שלנו $\boldsymbol{\theta}$, לערך התחלתי כל שהוא: $\boldsymbol{\theta}^{(0)}$
- חוזרים על צעד העדכון הבא עד להתכנסות:

  $$
  \boldsymbol{\theta}^{(t+1)}=\boldsymbol{\theta}^{(t)}-\eta \nabla_{\boldsymbol{\theta}}f(\boldsymbol{\theta}^{(t)})
  $$

הפרמטר $\eta$ אשר קובע את גודל הצעדים אשר נעשה בתהליך ההתכנסות. (את הדיון על קריטריון ההתכנסות ועל הבחירת של  $\eta$ נשאיר לשלב מאוחר יותר)

לדוגמא, בבעיה שלנו הגרדיאנט יהיה:

$$
\nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta};\mathcal{D})
=2\sum_{i=0}^N(\theta_1-\theta_2 \exp(-\theta_3 x)-y_i)\begin{bmatrix}
1\\
-\exp(-x/\theta_3)\\
-x\theta_2/\theta_3^2 \exp(-x/\theta_3)
\end{bmatrix}
$$

בעובר התחלה מהנקודה $\boldsymbol{\theta}^{(0)}=[1,1,1]^{\top}$ והתקדמות בכיוון הגרדיאנט (עם גודל צעד של $\eta=0.1$) נותנת את הפתרון הבא: 

<div class="imgbox">

![](../lecture01/output/fitting_exp_model.gif)

</div>

כפי שציינו קודם לאלגוריתם זה מספר בעיות בהם נדון בהמשך הקרוס. לדוגמא במקרה זה, בכדי לקבל זמן התכנסו סביר היינו צריכים לחלק את הערכים של $\boldsymbol{x}$ ב10 בכדי שיהיה מאותו סדר גודל של.

</div>
