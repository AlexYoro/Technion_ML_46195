---
type: lecture
index: 9
template: page
make_docx: true
print_pdf: true
---

<div dir="rtl" class="site-style">

# הרצאה 9 - CNN

<div dir="ltr">
<a href="./slides/" class="link-button" target="_blank">Slides</a>
<a href="/assets/lecture09.pdf" class="link-button" target="_blank">PDF</a>
<a href="./code/" class="link-button" target="_blank">Code</a>
</div>

## Stochastic and Mini-Batch Gradient Descent

צעד העדכון ב gradient descent נתון על ידי:

$$
\boldsymbol{\theta}^{(t+1)}=\boldsymbol{\theta}^{(t)}-\eta\nabla_{\boldsymbol{\theta}}g(\boldsymbol{\theta})
$$

כאשר $g(\boldsymbol{\theta})$ היא פונקציית ה objective שאותה אנו מעוניינים למזער. בהקשר של מערכות לומדות פונקציה זו תכיל לרוב סכום או ממוצע על כל הדגימות במדגם. בקרוס זה נתקלנו בשתי בעיות האופטימיזציה הבאות למציאת פרמטרים של מודל:

1. ב ERM אנו רוצים למזער את התוחלת האמפירית של ה risk בעבור חזאי פרמטרי כל שהוא $\hat{y}=h(\boldsymbol{x^{(i)}};\boldsymbol{\theta})$

    $$
    \underset{\boldsymbol{\theta}}{\arg\min} \underbrace{\frac{1}{N}\sum_{i=1}^N l(h(\boldsymbol{x^{(i)}};\boldsymbol{\theta}),y^{(i)})}_{g(\boldsymbol{\theta};\mathcal{D})}
    $$

2. MLE, כאשר אנו ממזערים את ה מינוס log-likelihood בעבור פונקציית פונקציית פילוג כל שהיא $p_{\text{y};\mathbf{x}}(y|\boldsymbol{x};\boldsymbol{\theta})$ (לקחנו כאן את המקרה של מקבל בגישה הדיסקרינימניטבית הסתברותית):

    $$
    \underset{\boldsymbol{\theta}}{\arg\min} \underbrace{\sum_{i=1}^N \log(p_{\text{y};\mathbf{x}}(y^{(i)}|\boldsymbol{x}^{(i)};\boldsymbol{\theta})}_{g(\boldsymbol{\theta};\mathcal{D})}
    $$

במקרים אלו גם הגרדיאנט יכיל סכום על כל המדגם. כאשר המדגם מאד גדול הסכימה יכולה להיות מאד בעייתית וארוכה לחישוב במקרים כאלה נרצה להשתמש בחישוב אלטרנטיבי אשר משתמש בכל צעד רק בחלק מן המדגם ולא בכולו.

### Stochastic Gradient Descent

Stochastic Gradient Descent מחשב בכל פעם את הנגזרת על פי **דגימה בודדת** מתוך המדגם, בלי סכימה בכלל, כאשר בכל צעד נשתמש בדגימה אחרת. שתי אופציות לבחירה של הדגימה בכל צעד הינן:

1. בכל צעד להגריל דגימה אקראית אחרת
2. לעבור על הדגימות במדגם צורה סידרתית (במקרה זה חשוב לרוב לערבב את הסדר של דגימות)

ההיגון מאחורי שיטה זו הינה שאומנם הנגזרת לפי כל אחת מהדגימות תצביע לכיוון שונה מהנגזרת של הסכום אבל בממוצע על פני כל הדגימות הכיוון הכללי יהיה זהה לכיוון של הנגזרת של הסכום.

היתרונות של שיטה זו הינה שהחישוב הוא מאד מהיר שכן במקום סכום על כל המדגם אנו צריכים לחשב את הנגזרת רק בעבור דגימה בודדת, אך החיסרום של שיטה זו הינה שהכיוון של הגרדיאנט יהיה מאד "רועש" ואנו נצטרך לעשות צעדים מאד קטנים שהאלגוריתם באמת יתקדם בכיוון הנכון.

### Mini-Batch Gradient Descent

Mini-batch gradient descent הוא פתרון ביניים בין stochastic gradient descent וה gradient descent הרגיל. בשיטה זו נשתמש בקבוצת דגימות מתוך המדגם המוכנה mini-batch על מנת לחשב את הנגזרת. בכל צעד אנו נחליף את ה mini-batch. גירסא זו של האלגוריתם היא הנפוצה ביותר לאימון של רשתות נוירונים כאשר גדלים אופיינים של ה mini-batch הינם 32-256 דגימות.

### שמות

- **Epoch**: כאשר אנו עוברים על המדגם באופן סידרתי עם stochastic gradient descent או עם mini-batch gradient descent אנו נגיד שהשלמנו epoch בכל פעם שנסיים מעבר מלא על כל המדגם והתחנו מהתחלה.
- למרות שברוב הספרי הלימוד רבים מתייחסים ל batch כאלה המדגם כולו, בפועל ביום יום משתמשים בשם batch כדי להתייחס ל mini-batch.
- החבילות של machine learning בהם ממומש אלגוריתם המימוש של gradient descent מופיע תחת השם stochastic gradient descent למרות שבפעול ניתן להשתמש בו לכל אחד מהמימושים שציינו (stochastic, mini-batch ורגיל).

## עצירה מוקדמת של gradient descent

מסתבר שדרך מוצלחת נוספת למנוע overfitting הינה לעצור את אלגוריתם הגרדיאנט לפני שהוא מתכנס. הדרך לעשות זאת הינה לבדוק את הערך של ה objective על ה validation set לאורך כל תהליך ההתכנסות של אלגוריתם ה gradient descent ולשמור תמיד בצד את הפרמטרים אשר נותנים את ה objective הנמוך ביותר על ה validation set.

## Convolutional Neural Networks (CNN)

בהרצאה הקודם הצגנו את ארכיטקטורת ה MLP. כפי שראינו ניתן להגדיל את היכולת הייצוג של הארכיטקטורה על ידי הגדלת הרשת (מספר השכבות והרוחב שלהם). הבעיה היא, שכפי שקורה בכל מודל פרמטרי, הגדלה של יכולת הייצוג תגדיל גם את ה overfitting שהמודל יעשה. באופן כללי רשת בעלת ארכיטקטורה טובה היא לאו דווקא רשת בעלת יכולת ייצוג גבוהה אלא דווקא רשת בעלת יכולת ייצוג נמוכה אשר עדיין מוסגלת לקרב בצורה טובה את הפונקציה שאותה היא מנסה למדל (למשל את החזאי האופטימאלי ב ERM או את הפילוג המותנה בגישה הדיסקרימינטיבת הסתברותית).

מסתבר שישנם בעיות רבות שבהם ארכיטקטורה אשר נקראת convolutional nerual network (CNN) עונה בדיוק על דרישות אלו. ארכיטקטורה זו מבוססת על שכבות הנקראות שכבות קונבולוציה. נסביר ראשית כיצד שכבות אלו פועלות ואחר כך נסביר לאילו מקרים הם טובות.

### שכבת קונבולוציה

שכבה קונבולוציה דומה לשכבת fully connected (FC) אך היא נבדלת ממנה בשני מובנים:

1. כל נוירון בשכבה זו מוזן רק מכמות מוגבלת של ערכים הנמצאים בסביבתו הקרובה (בשרטוט המוצג כל נוירון מוזן מ3 ערכים: זה שנמצא מולו, אחד לפני ואחד אחרי).
2. כל הנוירונים בשכבה מסויימת זהים, זאת אומרת שהם משתמשים באותם המשקלים (תכונה המכונה **weight sharing**).

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/conv.png)

</div>

שכבת קונבולוציה היא מקרה פרטי של שיכבת FC שבה כל הקשרים שלא מופיעים בשכבת הקונבולוציה הם 0 ושהמשקולות שלא התאפסו בכל נוירון הם בעות ערכים זהים בין כל הנורונים.

למעשה ניתן לחשוב על הפעולה שאותה מבצע הנוירון כאילו הוא נע לאורך הערכים שבכניסה לשיכבה ומפעיל את הפונקציה שלו כל פעם על סט ערכים אחר:

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/conv.gif)

</div>

(לשם הפשטות כאן סימנו את הכניסה לשכבה ב $\boldsymbol{x}$ ואת המוצא ב $\boldsymbol{y}$ והשמטנו את האינדקס של השכיבה $i$.)

מתמטית השיכבה מבצעת את שלושת הפעולות הבאות:

1. פעולת קרוס-קורלציה (ולא קונבולוציה) בין וקטור הכניסה $\boldsymbol{x}$ ווקטור משקולות $\boldsymbol{w}$ באורך $K$.
2. הוספת היסט $b$ (אופציונלי).
3. הפעלה של פונקציית הפעלה על וקטור המוצא איבר איבר.

פעולת הקרוס-קורלציה מוגדרת באופן הבא:

$$
y_i=\sum_{m=1}^K x_{i+m-1}w_m
$$

וקטור המשקולות של שכבת הקונבולציה $\boldsymbol{w}$ נקרא **גרעין הקונבולוציה (convolution kernel)**.

(שימו לב שבניגוד לשמה, שכבת הקונבולוציה מבצעת קורלציה ולא קונבולוציה. ההבדלים בין השתי הפעולות במקרה זה רק עניין של הדרך בה ממספרים את האיברים בוקטור $\boldsymbol{w}$, בקונבולוציה יש להפוך קודם את סדר האיברים בוקטור ורק אז לחשב את הקורלציה).

גדול המוצא של שכבת הקונבולוציה הוא קטן יותר מהכניסה והוא נתון על ידי $D_{\text{out}}=D_{\text{in}}-K+1$.

משום ששכבת הקונבולוציה היא מקרה פרטי מאד מצומצם של שכבת FC יכולת הייצוג שלה קטנה בהרבה. מקובל להסתכל על כמות הפרמטרים של מודל מסויים בתור הערכה גסה ליכולת הייצוג שלו. נשווה בין כמות הפרמטרים בשכבת FC ובשכבת קונבולוציה. נסתכל על שכבת קונבולוציה עם גרעין באורך $K=3$ הפועל על כניסה באורך 10. המוצא של שכבה זו יהיה באורך 8, בשיכבה יהיו ארבעה פרמטרים, שלושת המשקולות שבגרעין ועוד איבר היסט יחיד. לעומת זאת בשכבת FC המחברת כניסה באורך 10 עם מוצא באורך 8 יהיו $8\times10=80$ משקולות אשר קובעות את הקומבינציה הלינארית בכל נוירון ועוד 8 איברי היסט בעבור כל אחד מהנוירונים. ניתן לראות אם כן שבשכבת הקונבולוציה יש משמעותית הרבה פחות פרמטרים.

באופן כללי, בשכבת FC קיימות $D_{\text{in}}\times D_{\text{out}}$ משקולות ועוד $D_{\text{out}}$ איברי היסט. לעמות זאת, בשכבת קונבולציה יש $K$ משקולות ואיבר היסט בודד.

### קלט רב-ערוצי

במקרים רבים נרצה ששכבת הקונבולציה תקבל קלט רב ממדי, לדוגמא, תמונה בעלת שלושה ערוצי צבע או קלט שמע ממספר ערוצי הקלטה. מבנה זה מאפשר לאזור מרחבי בקלט להכיל אינפורמציה ממספר ערוצי כניסה.

במקרים אלו הניורון $h$ יהיה פונקציה של כל ערוצי הקלט:

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/conv_multi_input.gif)

</div>

הפונקציה $h$ היינה קומבינציה לינארית של כל ערוצי הקלט, לרוב מסיפים איבר הסט $b$ ופונקציית אקטיבציה.

### פלט רב-ערוצי

בנוסף, נרצה לרוב להשתמש ביותר מגרעין קונבולוציה אחד, במקרים אלו נייצר מספר ערוצים ביציאה בעבור כל אחד מגרעיני הקונבולוציה.

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/conv_multi_chan.gif)

</div>

בשכבות אלו אין שיתוף של משקולות בין ערוצי הפלט השונים, כלומר כל גרעין קונבולציה הוא בעל סט משקולות יחודי הפועל על כל הערוצי הכניסה על מנת להוציא פלט יחיד.
מספר הפרמטרים בשכבת כזאת היינו:  $\underbrace{C_\text{in}\times C_\text{out}\times K}_\text{the weights}+\underbrace{C_\text{out}}_\text{the bias}$.

כאשר:

- $C_\text{in}$ - מספר ערוצי קלט.
- $C_\text{out}$ - מספר ערוצי פלט.
- $K$ - גודל הגרעין.

### הרחבות נוספות שלשכבות הקונבולוציה

לרוב מרחיבים מעט את ההגדרה של שכבת הקונבולוציה הבסיסית שהצגנו על ידי הוספת התכונות הבאות:

#### Padding - ריפוד

משום שפעולת קונבולציה היינה מרחבית, בקצוות הקלט ישנה בעיה שאין ערכים חוקיים שניתן לבצע עליהם פעולות, לכן נהוג לרפד את שולי הקלט (באפסים או שכפול של אותו ערך בקצה)

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/padding.gif)

</div>

#### Stride - גודל צעד

ניתן להניח שלרוב הקשר המרחבי נשמר באזורים קרובים, לכן על מנת להקטין בחישוביות ניתן לדלג על הפלט ולהפעיל את פעולת הקונבולציה באופן יותר דליל. בפשטות: מדלגית על היציאות בגודל הצעד. לרוב גודל הצעד מסומן ב $s$, בדוגמא הבאה גודל הצעד היינו $s=2$. אלא אם רשום אחרת, גודל הצעד של השכבה הוא 1.

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/stride.gif)

</div>

#### Dilation - התרחבות

שוב על מנת להקטין בחישובית, אפשר לפעול על אזורים יותר גדולים תוך הנחה שערכים קרובים גיאוגרפית הם בעלי ערך זהה, על כן נרחיב את פעולת הקונבולציה תוך השמטה של ערכים קרובים. לרוב נסמן את ההתרחבות ב $d$ בדוגמא הבאה $d=2$. אלא אם רשום אחרת, ההתרחבות היא 1.

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/dilation.gif)

</div>

### Max / Average Pooling

שיכבות נוספת אשר מופיעה במקרים רבים ברשתות CNN הם שכבות מסוג pooling. שכבות אלו מחליפות את פעולת הקונבולוציה בפונקציה קבועה אשר מייצרת סקלר מתוך מתוך הקלט של הנוירון. שני שכבות pooling נפוצות הם max pooling ו average pooling, שכבה זו לוקחת את הממוצע או המקסימום של ערכי הכניסה.

דוגמא זו מציגה max pooling בגודל 2 עם גודל צעד (stride) גם כן של 2:

<div class="imgbox" style="max-width:400px">

![](../lecture09/assets/max_pooling.gif)

</div>

בשכבה זאת אין פרמטרים נלמדים.

### 2D Convolutional Layer

במרקים רבים נרצה לעבוד על קלט דו מימדי, לדוגמא על תמונות. במקרים כאלה הקונבולוציה תהיה דו מימדית. הגרפים הבאים מדגימים כיצד נראית פעולת שכבת הקונבולוציה על קלט דו מימדי (הירוק) אשר מייצרת פלט דו מימדי (הכחול) בעבור ערכים שונים של ה padding, stride ו dilation.

<table style="width:100%; table-layout:fixed;">
  <tr>
    <td><center>kernel size=3<br>padding=0<br>stride=1<br>dilation=1</center></td>
    <td><center>kernel size=4<br>padding=2<br>stride=1<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=1<br>stride=1<br>dilation=1<br>(Half padding)</center></td>
    <td><center>kernel size=3<br>padding=2<br>stride=1<br>dilation=1<br>(Full padding)</center></td>
  </tr>
  <tr>
    <td><img width="150px" src="../lecture09/assets/no_padding_no_strides.gif"></td>
    <td><img width="150px" src="../lecture09/assets/arbitrary_padding_no_strides.gif"></td>
    <td><img width="150px" src="../lecture09/assets/same_padding_no_strides.gif"></td>
    <td><img width="150px" src="../lecture09/assets/full_padding_no_strides.gif"></td>
  </tr>
  <tr>
    <td><center>kernel size=3<br>padding=0<br>stride=2<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=1<br>stride=2<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=1<br>stride=2<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=0<br>stride=1<br>dilation=2</center></td>
  </tr>
  <tr>
    <td><img width="150px" src="../lecture09/assets/no_padding_strides.gif"></td>
    <td><img width="150px" src="../lecture09/assets/padding_strides.gif"></td>
    <td><img width="150px" src="../lecture09/assets/padding_strides_odd.gif"></td>
    <td><img width="150px" src="../lecture09/assets/dilation_2d.gif"></td>
  </tr>
</table>

- \[1\] Vincent Dumoulin, Francesco Visin - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)([BibTeX](https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214))
  
### מבנה רשת CNN

רשתות קונבולוציה מורכבת משילוב של שכבות קנבולוציה, pooling ו FC. לדוגמא, אחת הרשתות הפופולריות היום לסיווג של תמונות הינה רשת בשם VGG-16. הרשת מקבלת תמונת צבע (3 ערוצים) בגודל 224x224 ומסווגת אותם ל 1 מ 1000 קטגוריות. הרשת נראית כך:

<div class="imgbox" style="max-width:900px">

![](./assets/vgg16.png)

</div>

(כל שכבות הקונבולוציה ברשת הם בלי stride או dilation, זאת אומרת stride=1 ו dilation=1, ועם padding של 0 אחד בכל שפה על מנת לשמור על הגודל של התמונה בשכבות הקונבולוציה)

### למה זה עובד?

הסיבה שרשתות קונבולוציה מתאימות למשימות של סיווג תמונות בגלל שתי התכונות של הבעיה המתאימות לתכונות של שכבות הקנובולוציה.

#### תלות של כל נוירון רק בסביבה המיידית שלו

התכונה הראשונה של שכבות קונבולוציה הינה שכל נוירון מוזן מהערכים בסביבה המיידית שלו. תכונה זו מתאימה לסיווג של תמונות משום שניתן באופן יעיל לנסות להבין מה האובייקט שמופיע בתמונה היא באופן היררכי, נדגים את המשמעות של זה על ידי רשת שמנסה לזהות האם בתמונה מסויימת מופיע פרצוף.

<div class="imgbox">

![](./assets/face.jpg)

</div>

גרעיני הקנובולוציה של השכבות הראשונות יעברו על התמונה ויחפשו, בעזרת קורלציה עם הגרעינים, תופעות בסיסיות כמו פסים אנכיים, פסים אופקיים, פינות, נקודות קטנות וכו'. כל גרעין ייצר ערוץ אשר מתאים לתופעה שאותה הוא מחפש. זאת אומרת שיהיה לנו ערוץ בעבור כל תופעה. לדוגמא הייצור של פסים אופקיים יעשה כך:

<div class="imgbox">

![](./assets/horizontal_filter.png)

</div>

שכבת קונבולוציה עם 4 ערוצים במוצא תראה כך:

<div class="imgbox">

![](./assets/conv_illustration.png)

</div>

לאחר שנעשה pooling ונקטין את התמונה פי 2, נוכל להשתמש בשכבות הבאות בכדי לחפש אובייקטים אשר מורכבים מהתועפות שמצאו השכבות הראשונות. לדוגמא נוכל לחפש איזורים שמכילים הרבה פסים אנכיים בכדי לזהות איזורים שעשויים להכיל שיער, או לדוגמא לחפש שני פסים אופקיים סמוכים שעשויים להכיל שפתיים וכו'. לאחר כל pooling שמקטין את התמונה (או קונבולוציה עם stride) נוכל לחפש אובייקטים הולכים וגדלים המורכבים מאובייקטים קטנים יותר.

שיטה זו שאנו מנסים להבין את תכולת התמונה על ידי שימוש בסדרה של שכבות כאשר בכל שיכבה מנסים להבין מה קורה באותו איזור רק על פי הסביבה המידית של אותו איזור בדיוק מתאימה לעובדה שבשכבת הקונבולוציה כל נוירון מוזן רק מהסביבה המיידית שלו.

#### Weight sharing

התכונה הנוספת של שכבת הקונבולוציה הינה שהמשקולות של כל הנוירונים משותפים בין כל הנוירונים באותו השכבה באותו ערוץ. ישנם מספר סיבות ללמה אילוץ זה לא מגביל מאד את היכולת לזהות אובייקטים:

1. הסיווג של התמונה לא אמור להיות מושפע אם מזיזים את האובייקט בתמונה מעט לצדדים. מהסיבה הזו אנו בעצם צריכים פונקציה שהיא בגדול איווריאנטית להזזות. בפועל זה אומר שאנו רוצים להפעיל את אותם הפעולות הלוקליות בשכבות הראשונות בצורה דומה בכל איזור בתמונה.

2. הפעולות שהשכבות הראשונות מבצעות, כגון חיפוש קווים אופקיים ואנכיים משותף לכל האובייקטים שנרצה לחפש בכל האיזורים בתמונה.

## Batch Normalization (לא למבחן)

אחת הבעיות בעבודה עם רשתות עמוקות הינה שיכול להיווצר מצב שבו הערכים במוצא של כל שכבה הם מסדר גודל שונה. הדבר מאד משפיע על הגרדיאנטים של כל שיכבה ויכול ליצור גרדיאנטים בטווח ערכים מאד גדול שמאד מקשה על הבחירה של גודל הצעד. אנו נרחיב על כך בתרגול בהקשר של האיתחול של הפרמטרים של הרשת באלגוריתם ה gradient descent.

דרך אחרת לנסות ולהבטיח כי המוצאים של כל שכבה יהיו בערך מאותו סדר גודל הינה על ידי הוספה של שכבה בשם batch normalization אשר מנסה לנרמל את הערכים אשר עוברים דרכה (מביאה את התוחלת של הערכים ל 0 ואת הסטיית תקן ל 1). הדרך שהיא עושה זאת הינה על ידי חישוב התוחלת וסטיית התקן האמפירית של הערכים על פני ה batch הספציפי באותו צעד גרדיאנט.

נסתכל על שכבת batch norm המקבלת וקטור $\boldsymbol{z}_{\text{in}}$ ומוציאה וקטור $\boldsymbol{z}_{\text{out}}$:

<div class="imgbox" style="max-width:400px">

![](./assets/batch_norm.png)

</div>

נניח כי בצעד עדכון מסויים אנו רוצים לחשב את הגרדיאנט של הרשת בעבור mini-batch מסויים $\{\boldsymbol{x}^{(i)}\}_{i=1}^M$. נניח כי הוקטורים המתקבלים בכניסה לשכבת ה batch norm הם $\{\boldsymbol{z}_{\text{in}}^{(i)}\}_{i=1}^M$. שיכבת ה batch norm תחשב את התוחלת וסטיית התקן האמפירית של הכניסה באופן הבא:

$$
\boldsymbol{\mu}=\frac{1}{M}\sum_{i=1}^M \boldsymbol{z}_{\text{in}}^{(i)}
$$

$$
\sigma^2=\frac{1}{M}\sum_{i=1}^M (\boldsymbol{z}_{\text{in}}^{(i)}-\boldsymbol{\mu})^2
$$

המוצא של השכבה יהיה:

$$
\boldsymbol{z}_{\text{out}}=\frac{
\boldsymbol{z}_{\text{in}}-\boldsymbol{\mu}
}{\sigma+\epsilon}
$$

כאשר $\epsilon$ הוא מספר קטן כל שהוא אשר אמור למנוע חלוקה ב 0.

לרוב השכבה תכיל גם טרנספורמציה לינרארית נלמדת עם פרמטרים $\gamma$ ו $\beta$:

$$
\boldsymbol{z}_{\text{out}}=\frac{
\boldsymbol{z}_{\text{in}}-\boldsymbol{\mu}
}{\sigma+\epsilon}\cdot\gamma+\beta
$$

כאשר $\gamma$ ו $\beta$ הוא וקטורים באורך של $\boldsymbol{z}$ והמכפלה עם $\gamma$ היא איבר איבר.

### אחרי שלב האימון

במהלך הלימוד מחזיקים ממוצע נע ([exponantial moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average)) של הערכים $\mu$ ו $\sigma$ ובסוף שלב הלימוד מקבעים את הערכים שלהם ואלו הערכים שבהם הרשת תשתמש לאחר שלב האימון.

</div>
