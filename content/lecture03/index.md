---
type: lecture
index: 3
template: page
make_docx: true
print_pdf: true
---

<div dir="rtl" class="site-style">

# הרצאה 3 - Generalization & overfitting

<div dir="ltr">
<a href="/assets/lecture03.pdf" class="link-button" target="_blank">PDF</a>
<!-- <a href="./code/" class="link-button" target="_blank">Code</a> -->
</div>

## רקע

בהרצאה הקודמת הצגנו את בעיית החיזוי שבה אנו מנסים לבנות חזאי על סמך מדגם. והגדרנו את ההמוגשים והסימונים הבאים:

- $\text{y}$ - ה labels - המשתנה האקראי שאותו אנו מנסים לחזות.
- $\mathbf{x}$ - ה measuments - הוקטור הארקאי שלפיו אנו מנסים לחזות.
- $\mathcal{D}=\{\boldsymbol{x}_i, y_i\}_{i=0}^N$ - ממדגם (dataset) אשר כולל $N$ זוגות בלתי תלויים של דגימות של $\mathbf{x}$ ו $\text{y}$.
- $\hat{y}$ - תוצאת חיזוי כל שהיא.
- $\hat{y}=h(\boldsymbol{x})$ - פונקציית החיזוי.
- $C(h)$ - פונקציית המחיר אשר נותנת "ציון" לכל חזאי.

הצגנו את הדרך הנפוצה להגדיר את פונקציית המחיר כתוחלת של איזו שהיא פונקציית הפסד:

$$
C(h)=R(h)=\mathbb{E}[l(h(\mathbf{x}),y)]
$$

כאשר $l$ היא פונקציית הפסד כל שהיא ו $R$ מכונה פונקציית הסיכון.

הגדרנו את החזאי האופטימאלי כחזאי בעל הציון (המחיר) הנמוך ביותר:

$$
h^*=\underset{h}{\arg\min}\ C(h)\left(=\underset{h}{\arg\min}\ \mathbb{E}[l(h(\mathbf{x}),\text{y})]\right)
$$

הבעיה עם פונקציית המחיר הזו הינה התוחלת על הפילוג לא ידוע. הצגנו בשם ERM אשר מתמודדת עם בעיה זו על ידי החלפת התוחלת בתוחלת אמפירית על המדגם

$$
h^*_{\mathcal{D}}=\underset{h}{\arg\min}\ \frac{1}{N}\sum_i l(h(\boldsymbol{x}_i),y_i)
$$

בנוסף ציינו שלרוב אנו נגביל את עצמו לחזאיים אשר מגיעים ממשפחה מצומצמת של חזאיים וציינו שהדרך הנפוצה לעשות זאת הינה על ידי שימוש במודל פרמטרי $h(\boldsymbol{x};\boldsymbol{\theta})$:

$$
\boldsymbol{\theta}^*_{\mathcal{D}}=\underset{\boldsymbol{\theta}}{\arg\min}\ \frac{1}{N}\sum_i l(h(\boldsymbol{x}_i;\boldsymbol{\theta}),y_i)
$$

ראנו בתרגול מספר דוגמאות לשימוש בשיטה זו כאשר $h(\boldsymbol{x};\boldsymbol{\theta})$ הוא מודל לינארי.

בהרצאה זו נדון בבעית ה overfitting הנובעת מבחירת החזאי על סמך המדגם, נסביר את החשיבות של השימוש במודל פרמטרי לצורך ההתמודדות עם הבעיה ונציג שיטה נוספת להתמודדות עם הבעיה בשם רגולריזציה.

## הכללה ובעיית ה overfitting

### הכללה (generalization)

> בעיית הלמידה בתחום של מערכות לומדות היא בעיית הכללה, שבה אנו מנסים על סמך דוגמאות להסיק מסקנות לגבי ההתנהגות הכללית של המערכת.

לדוגמא בבעיות supervised learning מטרה שלנו היא לבנות חזאי אשר יוכל לבצע חיזויים טובים על דגימות שלא ראינו לפני.

#### הערכת הביצועיים / יכולת ההכללה של חזאי

בכדי להעריך את יכולת ההכללה של החזאי שבנינו, זאת אומרת את הביצועיים של החזאי על דגימות כלליות שלא הופיעו בשלב הלימוד, נוכל להשתמש במדגם נוסף המכיל דגימות שונות מהמדגם שבו השתמשנו בשלב הלימוד. לשם כך עלינו להקצות מבעוד מועד חלק מתוך המדגם שנתון לנו לטובת הערכת הביצועיים של החזאי. זאת אומרת שאת המדגם (ה dataset) שלנו אנו נחלק לשני חלקים:

- **Train set** - מדגם שעלפיו אנו נבנה את חזאי. $\mathcal{D}_\text{train}$.
- **Test set** - המדגם שבו נשתמש על מנת להעריך את ביצועי החזאי. $\mathcal{D}_\text{test}$.

אשר אנו עובדים עם פונקציית מחיר מהצורה של פונקציית risk נוכל להעריך את ביצועי החזאי על ה test set בעזרת התוחלת האמפירית:

$$
\text{test score}=\frac{1}{N}\sum_{\boldsymbol{x}_i,y_i\in\mathcal{D}_{\text{test}}} l(h(\boldsymbol{x}_i),y_i)
$$

##### גדולו של ה test set

אנו נרצה לבחור את ה test set כך שיהיה מספיק גדול בכדי שההערכה של ביצועים תהיה כמה שיותר מדוייקת אך לא גדול מידי בכדי לשמור את ה train set כמה שיותר גדול. כאשר המדגם גדול מספיק לא תהיה לנו בעיה להפריש test set שהוא גדול מספיק אך עדיין מהווה אחוז קטן מכלל הדגימות. כאשר המדגם לא מאד גדול מקובל לפצל את המדגם ל 80% train ו 20% test.

### Overfitting (התאמת יתר)

> תופעת ה overfitting מתארת את המצב שבו המודל הנלמד לומד מאפיינים מסויימים אשר מופיעים רק במדגם ואינם הם אינם מייצגים את התכונות של הפילוג האמיתי שלפיו מפולגים המשתנים האקראיים אשר מהם נוצר המדגם במדגם. תופעה זו פוגעת ביכולת ההכללה של המודל.

נסתכל על האילוסטרציה הבאה:

<div class="imgbox" style="max-width:400px">

![](./assets/overfitting.png)

</div>

בדוגמא זו אנו מנסים לבנות חזאי על סמך הנקודות הכחולות אשר נדגמו מתוך הפילוג האדום. החזאי האופטימאלי אשר מקטין את שגיאת החיזוי בהתייחס לכלל הפילוג הינו החזאי אשר עובר במרכז הפילוג. כאשר אנו מתייחסים רק על הנקודות הכחולות החזאי האופטימאלי יהיה חזאי אשר עובר דרך כל הנקודות הכחולות ומשיג שגיאת חיזוי 0 על נקודות אלה.

במרחב החזאים הדבר נראה כך:

<div class="imgbox" style="max-width:600px">

![](../lecture02/assets/models_diagram.png)

</div>

## הגבלת המודל ופירוק שיגאת החיזוי

כאשר אנו לא מגבילים את צורתו של החזאי אנו למעשה מאפשרים לו לקבל כל צורה שהיא כל עוד הוא עובר בין הנקודות של המדגם.בכדי לשלוט בצורה שזה הוא מתנהג נוכל להגביל את המרחב שבו אנו מחפשים את החזאים למשפחה מצומצמת של חזאיים אשר מתנהגים בצורה רצויה. כפי שציינו בהרצאה הקודמת אנו לבור נעשה זאת על ידי שימוש במודל פרמטרי.

נשתמש בהגדרה של שני החזאי הבאים (אשר הופיעו גם בהרצאה הקודמת):

- $h^*(\boldsymbol{x};\boldsymbol{\theta})$: החזאי ה**פרמטרי** האופטימאלי. החזאי בעל הביצועים הטובים ביותר (ממזער את פונקציית המחיר) מבין כל החזאים במשפחה הפרמטרית.

- $h^*_{\mathcal{D}}(\boldsymbol{x};\boldsymbol{\theta})$: החזאי המושערך: החזאי אשר נבנה על סמך מדגם מסויים $\mathcal{D}$.

נוסיף את שני החזאי האלו לשרטוט ממקודם:

<div class="imgbox" style="max-width:600px">

![](../lecture02/assets/models_diagram_non_parametric.png)

</div>

### יכולת הביטוי של המודל הפרמטרי

כאשר עובדים עם מודלים פרמטריים אנו נעסוק הרבה ב**יכולת הביטוי (expressiveness)** של המודל. אנו נשתמש במוגש זה על מנת לתאר עד כמה גדול מרחב הפונקציות שאותו יכול מודל פרמטרי מסויים לייצג. בקורס זה אנו נשתמש במודל זה בצורה איכותית ולא כמותית:

- כאשר מודל פרמטרי ידע לייצג משפחה מאד קטנה של מודלים, אנו נאמר שיש לו יכולת ביטוי נמוכה. לדוגמא: המודל הלינארי.
- כאשר מודל פרמטרי ידע לייצג **או לקרב בצורה טובה** מגוון רחב של מודלים אנו נאמר שיש לו יכולת ביטוי גבוה. לדוגמא: פולינום מסדר מאד גבוהה.

מצד אחד אנו נרצה מודל עם יכולת ביטוי גבוה על מנת שיוכל לקרב בצורה טובה את החזאי האידאלי, אך מצד שני יכולת יצוג גבוה תאפשר גם הרבה overfitting. בכדי להבין טוב יותר את ההשפעה של יכולת הביטוי של המודל נסתכל על הפירוק הבא של הגורמים המשפיעים על שגיאת החיזוי.

### Aprroxiamtion-estimation decomposition

כאשר עובדים עם משפחה מצומצת של מודל ניתן להתייחס לשני גורמים אשר מונעים מאיתנו למצוא את החזאי האופטימאלי $h^*(\boldsymbol{x})$.

1. **Approximation error - שגיאת קירוב**: השגיאה עקב ההגבלה של המודל למשפחה מצומצמת של מודלים (לרוב למודל פרמטרי). שגיאה זו נובעת מההבדל בין החזאי האופטימאלי $h^*(\boldsymbol{x})$ לבין החזאי **הפרמטרי** האופטימאלי $h^*(\boldsymbol{x},\boldsymbol{\theta})$.
2. **Estimation error - שגיאת השיערוך**: השגיאה הנובעת מהשימוש במדגם כתחליף לפילוג האמיתי וחוסר היכולת שלנו למצוא את המודל הפרמטרי האופטימאלי. שגיאה זו נובעת מההבדל בין המודל הפרמטרי האופטימאלי $h^*(\boldsymbol{x},\boldsymbol{\theta})$ למודל הפרמטרי המשוערך על סמך המדגם $h_{\mathcal{D}}^*(\boldsymbol{x},\boldsymbol{\theta})$.

<div class="imgbox" style="max-width:600px">

![](./assets/models_diagram_approx_estim_decomp.png)

</div>

שני השגיאות הנ"ל הם הגורמים להבדל בין החזאי המשוערך לחזאי האופטימאלי, אך כאשר נרצה לדבר על השגיאה הכוללת נרצה להתייחס להבדל בין החיזוי של החזאי המשוערך לבין $y$ עצמו. במקרים כאלה נוסיף לשגיאה גם את שגיאת החיזוי שאותה יעשה החזאי האופטימאלי:

3. **Noise - ה"רעש" של התויות**: השגיאה שהחזאי האופטימאלי צפוי לעשות. שגיאה זו נובעת מהאקראיות של התויות $y$.

#### Approximaion-estimation Tradeoffs: קביעת יכול הביטוי של המודל הפרמטרי

בעזרת פירוק זה של השגיאה נוכל לנסות להבין את השיקולים הקיימים בבחירה של יכולת הביטוי של המודל הפרמטרי. ככל שיכולת הביטוי של המודל תהיה גדולה יותר כך המרחק בין $h*(\boldsymbol{x};\boldsymbol{\theta})$ לבין $h*(\boldsymbol{x})$ יקטן ושגיאת הקירוב תקטן. הבעיה היא שלרוב ככל שיכולת הביטוי גדלה כך גדלה גם שיגאת השיערוך. נציג זאת הגרף הסכימתי הבא:

<div class="imgbox" style="max-width:600px">

![](./assets/approx_estim_tradeoff.png)

</div>

בשני קצוות הגרף הנקבל שגיאה כוללת מאד גדולה ומטרתינו תהיה למצוא את נקודת הפשרה בין שני הקצוות שבה השגיאה הכוללת היא הקטנה ביותר. תלות זו ביכולת הביטוי ביטוי של המודל ידועה בשם **approxiamtion-estimation tradeoff**.

### Bias-variance decomposition

פירוק ה approximation-estiamtion הוא פירוק רעיוני אשר מתאר את הגורמים השונים לשגיאה. במקרה הספציפי שבו פונקציית המחיר בבעיה הינה MSE ניתן להראות כי להראות כי ניתן לפרק את שגיאת ה MSE לסכום של שלושה אברי שגיאה אשר מקבילים לשלושת האיברים בפירוק ה approximation-estimation. לפני שנראה את הפירוק עצמו נגדיר ראשית חזאי נוסף אותו נכנה החזאי הממוצע.

#### המדגם כמשתנה אקראי והחזאי הממוצע

לצורך פירוק זה נרצה להתייחס לשגיאת ה MSE שתיתקבל משימוש בשיטה / מודל פרמטרי מסויים. נשים לב אבל שמכיוון שהחזאי המשוערך שאיתו נעבוד תלוי במדגם שבידינו אנו גם שגיאת ה MSE תהיה תלויה במדם. זאת אומרת שאם נחזור על התהליך של בניית החזאי ושיערוך השגיאה עם שני מדגמים שונים נקבל תוצאות שונות.

באופן כללי ניתן להסתכל על המדגם כעל משתנה אקראי שכן הוא מיוצר על ידי $N$ הגרלות של דגימות מתוך הפילוג. משום שהמדגם אקראי כך יהיו גם החזאי ושגיאת ה MSE. בכדי להסיר את התלות במדגם נסתכל על השגיאת MSE הממוצעת אשר מתקבלת לאחר לקיחה של התוחלת על כל המדגמים האפשריים.

$$
\text{average MSE}=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}\left[(h_{\mathcal{D}}(\mathbf{x})-\text{y})^2\right]\right]
$$

לשם הבהירות, אנו נשתמש בסימון $\mathbb{E}_\mathcal{D}$ בכדי לציין תוחלת על פני המדגמים האפשריים. (תוחלת ללא סימון $\mathbb{E}$ תהיה לפי $\mathbf{x}$ ו $\text{y}$). כמובן שלא ניתן בפועל לחשב את התחולת על פני כל החזאים השונים, אך כלי זה ישמש אותו לשם ההבנה של הגורמים לשגיאת החיזוי.

נגדיר את החזאי הממוצא כחזאי אשר מחזיר את החיזוי השהוא הממוצא על פני כל החזאיים אשר נבנו ממדגמים שונים:

$$
\bar{h}(x)=\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(x)\right]
$$

### הפירוק

על ידי שימוש בהגדרה של החזאי מממוצע ניתן להראות כי את שגיאת ה MSE הממוצעת על פני המדגמים ניתן לרשום כסכום על שלוש שגיאות:

כאשר $h^*(x)=\mathbb{E}\left[\text{y}|x\right]$ הוא החזאי האופטימאלי של בעיית החיזוי.

$$
\mathbb{E}_{\mathcal{D}}\left[
    \mathbb{E}\left[(h_{\mathcal{D}}(\text{x})-y)^2\right]
\right]
=
\mathbb{E}\left[
    \underbrace{\mathbb{E}_{\mathcal{D}}\left[(h_{\mathcal{D}}(\text{x})-\bar{h}(\text{x}))^2\right]}_{\text{Variance}}
    +\underbrace{(\bar{h}(\text{x})-h^*(\text{x}))^2}_{\text{Bias}^2}
    +\underbrace{(h^*(\text{x})-y)^2}_{\text{Noise}}
\right]
$$

בפירוק הזה:

- ה **variance** מודד את השונות של החזאים השונים המתקבלים ממדגמים שונים סביב החזאי הממוצע. זהו האיבר היחיד בפירוק אשר תלוי בפילוג של המדגם.
- ה **bias** מודד את ההפרש הריבועי בין החיזוי של החזאי הממוצע לבין החיזוי של החזאי האופטימאלי.
- ה **noise** (בודמה לפירוק הקודם) מודד את השגיאה הריבועית המתקבלת בעבור החיזוי האופטימאלי (אשר נובעת מהאקראיות של $y$).

בתרגול 4 אנו נראה את הפיתוח של פירוק זה.

בדומה ל approximation-estimation tradeoff ישנו גם **bias-variance tradeoff**

<div class="imgbox" style="max-width:600px">

![](./assets/bias_variance_tradeoff.png)

</div>

<!-- ## בחירת המודל הפרמטרי

אומנם המגבלה על החזאי תמנע לרוב מהמודל לייצג את החזאי האופטימאלי אך היא גם תמנע ממנו מלייצג הרבה פונקציות אשר מתנהגות בצורה לא רצויה. ההגדרה של מהיא צורה רצויה או לא תלויה בידע המקדים שיש לנו על המערכת. שתי דוגמאות להתנהגויות שהם לרוב רצויות מההכרות עם ההתנהגות של העולם באופן כללי:

- אנו נצפה שהחזאי שנקבל יתנהג בצורה "חלקה" יחסית כך של $\boldsymbol{x}$-ים קרובים נקבל חיזויים יחסית קרובים.
- אנו נצפה שגם המגמה הכללית של החזאי לא תשתנה בצורה מהירה.

בהתאם להתנהגות הצפויה של החזאי אנו נבחר משפחה מצומצמת של חזאים שמקיימים את התכונות הרצויות. ככל שיהיה בידינו יותר מידע על ההתנהגות הצפויה של המערכת נוכל לבחור את המשפחה המצומצמת בצורה טובה יותר. דוגמא למקרה שבו יש לנו הרבה מידע על ההתנהגות של המערכת היא הדוגמא מהתרגול הראשון שבו ניסינו להתאים מעגל לסט של נקודות שידענו שיושבות קרוב למעגל מסויים. -->

## Hyper-parameters וסדר המודל

על מנת מצוא את המשפחה הפרמטרית נרצה לבדוק סדרה של מודלים בעלי יכולת ביטוי אשר הולכת וגדלה. לדוגמא נרצה לבדוק פולינומים מסדר הולך וגדל על מנת מצוא את הסדר בעל יכולת ההכללה הטובה ביותר. לפני שנתאר את האופן שבו ניתן למצוא את הסדר הפולינום האופטימאלי נסביר מהם hyper-parameters.

### Hyper-parameters

Hyper parameters הינו שם כולל לכל הפרמטריים שמופיעים בשיטה או במודל הפרמטרי שבהם אנו משתמשים לבניית החזאי ואינם חלק מהפרמטרים שעליהם אנו מבצעים את האופטימיזציה. פרמטרים יכולים להיות לדוגמא:

- סדר הפולינום שבו אנו משתמשים.
- הפרמטר $\eta$ אשר קובע את גודל הצעד באלגוריתם ה gradient descent.
- פרמטרים אשר קובעים את המבנה של רשת נוירונים.

במקרים רבים יהיה לנו hyper-parameter אחד או יותר אשר שולט ביכולת הביטוי של המודל הפרמטרי, כדוגמאת המקרה של סדר הפולינום שבו נשתמש. במקרים כאלה פרמטרים אלו ב**סדר של המודל**.

### בחירת hyper-parameters בעזרת validation set

מכיוון שה hyper-parameters אינם חלק מבעיית האופטימיזציה אנו צריכים דרך אחרת לקבוע אותם. לרוב לנאלץ לקבוע את הפרמטרים האלו בשיטה של ניסוי וטעיה. זאת אומרת שהיה עלינו פשוט נסות ערכים שונים ובדוק את ביצועי המודל בעבור אותם ערכים.

מכיוון שאנו לא יכולים להשתמש ב test set בכדי לבנות את המודל שלנו אנו צריכים ליייצר מדגם ניפרד נוסף שעליו נוכל לבחון את ביצועי המודל בעבור ערכים שונים של ה hyper-parameters. אנו נייצר מדגם זה על ידי חלוקה נוספת של ה train set. למדגם הנוסף נקרא validation set.

במקרים רבים לאחר קביעת ה hyper-parmaeters אנו נאחד חזרה את ה validation set וה train set ונאמן מחדש את המודל על המדגם המאוחד (כל הדגימות מלבד ה test set).

## רגולריזציה

דרך אלטרנטיבית להקטנת הקטנת שגיאת השיערוך (או ה variance) הינה בעזרת כלי אשר נקרא **רגולריזציה (regularization)**. הרעיון בשימוש ברגולריזציה הינו להתערב בבעיית האופטימיזציה שאותה אנו מנסים לפתור ולגרום לבעיית האופטימיזציה "להעדיף" מודלים מסויימים על פני מודלים אחרים. דבר זה נעשה על ידי הוספת איבר נוסף המכונה **איבר רגולריזציה** לבעיית האופטימיזציה אשר נותן קנס גבוהה על שימוש במודלים מסויימים וקנס קטן יותר על מודלים אחרים. על ידי השימוש ברגולריזציה אנו למעשה מגבילים בצורה "רכה" את בעיית האופטימיזציה לסט מצומצם יותר של מודלים ועל ידי כך מקטינים את שגיאת השיערוך ומקטין מעט את הצורך להגביל את סדר המודל.

על מנת להוסיף רגולריזציה לבעיית האופטימיזציה עלינו לבחור פונקציה אשר מקבלת את הפרמטרים $\boldsymbol{\theta}$ של מודל מסויים ומחזירה את הקנס שאותו יש לתת למודל זה. לדוגמא, כמה רגולריזציות נפוצות נותנות קנס גדול יותר ככל שהפרמטרים של המודל גדולים יותר ובכך גורמת לבעיית האופטימיזציה "להעדיף" פתרונות על פרמטרים קטנים. את איבר הרגולריזציה אנו נוסיף לרוב לבעיית האופטימיזציה יחד עם קבוע כפלי $\lambda$ אשר יקבע את עוצמת (או משקל) הרגולריזציה באופן הבא:

$$
\boldsymbol{\theta}=\underset{\boldsymbol{\theta}}{\arg\min}\underbrace{f(\boldsymbol{\theta})}_{\text{The regular objective function}}+\lambda\underbrace{g(\boldsymbol{\theta})}_{\text{The regularization term}}
$$

המשקל אותו אנו נותנים לרגולריזציה $\lambda$ הוא hyper-parameter של האלגוריתם שאותו יש לקבוע בעזרת ה validation set.

הבחירה של פונקציית הרגולריזציה $g(\theta)$ היא בחירה קשה ותלויה באופי של הבעיה אותה אנו מנסים לפתור. במרבית המקרים הבחירה נעשית בשיטה של ניסוי טעיה על פונקציות רגולריזציה נפוצות. שני הרגולריזציות הנפוצות ביותר הינן:

- $l_1$ - אשר מוסיפה איבר רגולריזציה של $g(\boldsymbol{\theta})=\lVert\boldsymbol{\theta}\rVert_1$.
- $l_2$ - (Tikhonov regularizaion) אשר מוסיפה איבר רגולריזציה של $g(\boldsymbol{\theta})=\lVert\boldsymbol{\theta}\rVert_2^2$.

רגולריזציות אלו מנסות לשמור את הפרמטריים כמה שיותר קטנים. המוטיבציה מאחורי הרצון לשמור את הפרמטרים קטנים הינה העובדה שבמרבית המודלים ככל שהפרמטרים קטנים יותר המודל הנלמד יהיה בעל נגזרות קטונות יותר ולכן הוא ישתנה לאט יותר ופחות "ישתולל".

### ההבדל בין $l_1$ ו $l_2$

משום שהקנס ב $l_2$ גדל בצורה ריבועית עם הפרמטרים גודלו של הקנס יקבע בעיקר לפי הפרמטרים הגדולים של המודל ולפרמטרים והם אלו שיהיו מושפעים מהתוספת של הרגולריזציה.מכיוון שהרגולריזציה תתמקד בעיקר בלהקטין את הפרמטרים שגדולים יותר מהאחרים היא למעשה תנסה בפועל לשאוף שכל הפרמטרים יהיו קטנים אך באופן יחסית אחיד.

מנגד $l_1$ תפעל להקטין את כל האיברים כמה שיותר ללא קשר לגודלם. לדוגמא להקטנה של פרמטר מ2 ל-1 יהיה אותו אפקט כמו הקטנה של פרמטר מ100 ל-99. התוצאה בפועל הינה שרגולריזציית $l_1$ תגרום לפרמטרים הפחות חשובים להתאפס. במקרים רבים וקטור הפרמטרים שיתקבל מפתרון של בעיה שם רגולריזציית $l_1$ יכיל הרבה מאד אפסים. וקטורים כאלה מכולים לרוב וקטורים דלילים (sparse).

### דוגמא: בעיות LLS עם רגולריזציה

נדגים כיצג נראת בעיית ה LLS אותה ראינו בהרצאה הקודמת כאשר מוסיפים רגולריציית $l_1$ ו $l_2$:

#### Ridge regression: LLS + $l2$ regularization

$$
\boldsymbol{\theta}=\underset{\boldsymbol{\theta}}{\arg\min}\frac{1}{N}\sum_i(\boldsymbol{x}^{(i)\top}\boldsymbol{\theta}-y^{(i)})+\lambda\lVert\boldsymbol{\theta}\rVert_2^2
$$

גם לבעיה זו יש פתרון סגור והוא נתון על ידי:

$$
\boldsymbol{\theta}^*=(X^{\top}X+\lambda)^{-1}X^{\top}\boldsymbol{y}
$$

אנו נראה את הפתוח של פתרון זה בתרגיל 4.2.

##### LASSO: LLS + $l1$ regularization

(LASSO = Linear Absolute Shrinkage and Selection Opperator)

$$
\boldsymbol{\theta}=\underset{\boldsymbol{\theta}}{\arg\min}\frac{1}{N}\sum_i(\boldsymbol{x}^{(i)\top}\boldsymbol{\theta}-y^{(i)})+\lambda\lVert\boldsymbol{\theta}\rVert_1
$$

לבעיה זו אין פתרון סגור ויש צורך להשתמש באלגוריתמים איטרטיביים כגון gradient descent.

</div>
